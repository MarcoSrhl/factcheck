{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Automated Fact-Checking System - Demo\n\nThis notebook demonstrates the full fact-checking pipeline:\n1. **Triplet Extraction** - Extract (subject, predicate, object) from claims\n2. **Entity Linking** - Map entities to DBpedia URIs\n3. **Knowledge Base Query** - Verify claims against DBpedia\n4. **Neural Classification** - BERT-based verdict prediction\n5. **Final Verdict** - SUPPORTED / REFUTED / NOT ENOUGH INFO\n6. **Explainability** - T5-generated explanations, KB reasoning chains, attention analysis, confidence decomposition"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "from src.triplet_extractor import TripletExtractor\n",
    "from src.entity_linker import EntityLinker\n",
    "from src.knowledge_query import KnowledgeQuery\n",
    "from src.fact_checker import FactChecker, format_result\n",
    "\n",
    "print('All modules loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Triplet Extraction\n",
    "\n",
    "We use spaCy dependency parsing to extract (subject, predicate, object) triplets from English sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = TripletExtractor()\n",
    "\n",
    "sentences = [\n",
    "    \"Paris is the capital of France\",\n",
    "    \"Barack Obama was born in Hawaii\",\n",
    "    \"Albert Einstein developed the theory of relativity\",\n",
    "    \"The Eiffel Tower is located in Paris\",\n",
    "    \"Tokyo is the capital of Japan\",\n",
    "]\n",
    "\n",
    "for sent in sentences:\n",
    "    triplets = extractor.extract(sent)\n",
    "    print(f'\\n\"{sent}\"')\n",
    "    for s, p, o in triplets:\n",
    "        print(f'  Subject: {s}')\n",
    "        print(f'  Predicate: {p}')\n",
    "        print(f'  Object: {o}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entity Linking\n",
    "\n",
    "Map extracted entities to their DBpedia URIs using the DBpedia Lookup API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linker = EntityLinker()\n",
    "\n",
    "entities = [\"Paris\", \"France\", \"Barack Obama\", \"Hawaii\", \"Eiffel Tower\", \"Albert Einstein\", \"Tokyo\", \"Japan\"]\n",
    "\n",
    "for entity in entities:\n",
    "    uri = linker.link(entity)\n",
    "    print(f'{entity:20s} -> {uri}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Knowledge Base Query\n",
    "\n",
    "Verify relations between entities using DBpedia SPARQL and JSON endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kq = KnowledgeQuery()\n",
    "\n",
    "pairs = [\n",
    "    (\"http://dbpedia.org/resource/Paris\", \"http://dbpedia.org/resource/France\"),\n",
    "    (\"http://dbpedia.org/resource/Barack_Obama\", \"http://dbpedia.org/resource/Hawaii\"),\n",
    "    (\"http://dbpedia.org/resource/Eiffel_Tower\", \"http://dbpedia.org/resource/Paris\"),\n",
    "    (\"http://dbpedia.org/resource/Tokyo\", \"http://dbpedia.org/resource/Japan\"),\n",
    "]\n",
    "\n",
    "for subj, obj in pairs:\n",
    "    result = kq.verify_triplet(subj, obj)\n",
    "    subj_name = subj.split('/')[-1].replace('_', ' ')\n",
    "    obj_name = obj.split('/')[-1].replace('_', ' ')\n",
    "    print(f'\\n{subj_name} <-> {obj_name}')\n",
    "    print(f'  Found: {result[\"found\"]} (via {result[\"method\"]})')\n",
    "    for p in result['predicates'][:3]:\n",
    "        print(f'  Predicate: {p.split(\"/\")[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Full Pipeline - Fact Checking\n",
    "\n",
    "Run the complete pipeline on 10 example claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full pipeline (with neural model if available, otherwise KB-only)\n",
    "import os\n",
    "model_path = '../models/fact_checker'\n",
    "use_neural = os.path.exists(model_path)\n",
    "checker = FactChecker(model_path=model_path if use_neural else None, use_neural=use_neural)\n",
    "print(f'Pipeline loaded (neural model: {\"enabled\" if use_neural else \"disabled - KB only\"})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = [\n",
    "    # True claims\n",
    "    \"Paris is the capital of France\",\n",
    "    \"Barack Obama was born in Hawaii\",\n",
    "    \"The Eiffel Tower is located in Paris\",\n",
    "    \"Albert Einstein developed the theory of relativity\",\n",
    "    \"Tokyo is the capital of Japan\",\n",
    "    # False claims\n",
    "    \"The Earth is flat\",\n",
    "    \"Napoleon was born in England\",\n",
    "    \"Mars is the largest planet in the solar system\",\n",
    "    # Ambiguous claims\n",
    "    \"Chocolate causes acne\",\n",
    "    \"Dogs can sense earthquakes before they happen\",\n",
    "]\n",
    "\n",
    "expected = [\n",
    "    \"SUPPORTED\", \"SUPPORTED\", \"SUPPORTED\", \"SUPPORTED\", \"SUPPORTED\",\n",
    "    \"REFUTED\", \"REFUTED\", \"REFUTED\",\n",
    "    \"NOT ENOUGH INFO\", \"NOT ENOUGH INFO\",\n",
    "]\n",
    "\n",
    "results = []\n",
    "for claim in claims:\n",
    "    result = checker.check(claim)\n",
    "    results.append(result)\n",
    "    print('=' * 60)\n",
    "    print(format_result(result))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metrics\n",
    "\n",
    "Evaluate the pipeline's performance against expected verdicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "predicted = [r['verdict'] for r in results]\n",
    "\n",
    "print('Claim-by-claim results:')\n",
    "print(f'{\"Claim\":50s} {\"Expected\":18s} {\"Predicted\":18s} {\"Match\"}')\n",
    "print('-' * 100)\n",
    "for claim, exp, pred in zip(claims, expected, predicted):\n",
    "    match = 'OK' if exp == pred else 'MISS'\n",
    "    print(f'{claim:50s} {exp:18s} {pred:18s} {match}')\n",
    "\n",
    "# Overall metrics\n",
    "labels = ['SUPPORTED', 'REFUTED', 'NOT ENOUGH INFO']\n",
    "acc = accuracy_score(expected, predicted)\n",
    "print(f'\\n{\"=\" * 50}')\n",
    "print(f'Accuracy: {acc:.2%}')\n",
    "print(f'\\nClassification Report:')\n",
    "print(classification_report(expected, predicted, labels=labels, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence distribution\n",
    "print('\\nConfidence distribution by verdict:')\n",
    "for verdict in labels:\n",
    "    confs = [r['confidence'] for r in results if r['verdict'] == verdict]\n",
    "    if confs:\n",
    "        avg_conf = sum(confs) / len(confs)\n",
    "        print(f'  {verdict:18s}: avg={avg_conf:.3f}, min={min(confs):.3f}, max={max(confs):.3f} (n={len(confs)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Full Pipeline with Explainability\n\nLoad the pipeline with the T5-based explainer to generate natural language explanations for each verdict."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the pipeline with explainability\nexplainer_path = '../models/explainer'\nuse_explainer = os.path.exists(explainer_path)\n\nchecker_full = FactChecker(\n    model_path=model_path if use_neural else None,\n    use_neural=use_neural,\n    use_explainer=use_explainer,\n    explainer_model_path=explainer_path,\n)\nprint(f'Full pipeline loaded (neural: {\"ON\" if use_neural else \"OFF\"}, explainer: {\"ON\" if use_explainer else \"OFF\"})')"
  },
  {
   "cell_type": "code",
   "source": "# Run the full pipeline with explainer on all claims\nresults_full = []\nfor claim in claims:\n    result = checker_full.check(claim)\n    results_full.append(result)\n    print('=' * 60)\n    print(format_result(result))\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Performance Comparison: Base vs Full Pipeline\n\nCompare the base pipeline (KB + BERT) against the full pipeline (KB + BERT + Explainer).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "predicted_base = [r['verdict'] for r in results]\npredicted_full = [r['verdict'] for r in results_full]\n\nacc_base = accuracy_score(expected, predicted_base)\nacc_full = accuracy_score(expected, predicted_full)\n\nprint('=' * 80)\nprint(f'{\"PERFORMANCE COMPARISON\":^80}')\nprint('=' * 80)\n\n# Claim-by-claim comparison\nprint(f'\\n{\"Claim\":42s} {\"Expected\":15s} {\"Base\":15s} {\"Full\":15s}')\nprint('-' * 87)\nfor claim_text, exp, pb, pf in zip(claims, expected, predicted_base, predicted_full):\n    base_mark = 'OK' if exp == pb else 'MISS'\n    full_mark = 'OK' if exp == pf else 'MISS'\n    print(f'{claim_text:42s} {exp:15s} {pb:10s} {base_mark:4s} {pf:10s} {full_mark}')\n\nprint(f'\\n{\"=\" * 80}')\nprint(f'Base pipeline accuracy:  {acc_base:.2%}')\nprint(f'Full pipeline accuracy:  {acc_full:.2%}')\nprint(f'{\"=\" * 80}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Classification reports side by side\nprint('BASE PIPELINE - Classification Report')\nprint(classification_report(expected, predicted_base, labels=labels, zero_division=0))\n\nprint('\\nFULL PIPELINE (with Explainer) - Classification Report')\nprint(classification_report(expected, predicted_full, labels=labels, zero_division=0))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Confidence distribution comparison\nprint('CONFIDENCE DISTRIBUTION')\nprint('=' * 70)\n\nfor pipeline_name, res_list in [('Base', results), ('Full', results_full)]:\n    print(f'\\n--- {pipeline_name} Pipeline ---')\n    for verdict in labels:\n        confs = [r['confidence'] for r in res_list if r['verdict'] == verdict]\n        if confs:\n            avg_conf = sum(confs) / len(confs)\n            print(f'  {verdict:18s}: avg={avg_conf:.3f}, min={min(confs):.3f}, max={max(confs):.3f} (n={len(confs)})')\n        else:\n            print(f'  {verdict:18s}: (no predictions)')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Detailed Explanation Showcase\n\nFull multi-layered explanations for 3 representative claims: one SUPPORTED, one REFUTED, one NOT ENOUGH INFO.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from src.explainer import FactExplainer\n\n# Use the explainer from the full checker if available, otherwise create one\nif checker_full.explainer is not None:\n    explainer_obj = checker_full.explainer\nelse:\n    explainer_obj = FactExplainer(use_t5=use_explainer, t5_model_path=explainer_path, use_attention=True)\n\n# Pick 3 representative results from the full pipeline\nshowcase_indices = [0, 5, 8]  # Paris/France (SUPPORTED), Earth flat (REFUTED), Chocolate/acne (NEI)\n\nfor idx in showcase_indices:\n    r = results_full[idx]\n    print('#' * 70)\n    print(f'# CLAIM: {r[\"claim\"]}')\n    print(f'# VERDICT: {r[\"verdict\"]} (confidence: {r[\"confidence\"]:.2f})')\n    print('#' * 70)\n\n    explanation = r.get('explanation')\n    if explanation is None:\n        # Generate explanation if not already in the result\n        explanation = explainer_obj.explain(r, classifier=checker_full.classifier)\n\n    print(explainer_obj.format_explanation(explanation))\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}